"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var execute_exports = {};
__export(execute_exports, {
  toolsAgentExecute: () => toolsAgentExecute
});
module.exports = __toCommonJS(execute_exports);
var import_messages = require("@langchain/core/messages");
var import_runnables = require("@langchain/core/runnables");
var import_agents = require("langchain/agents");
var import_omit = __toESM(require("lodash/omit"));
var import_n8n_workflow = require("n8n-workflow");
var import_node_assert = __toESM(require("node:assert"));
var import_helpers = require("../../../../../../utils/helpers");
var import_N8nOutputParser = require("../../../../../../utils/output_parsers/N8nOutputParser");
var import_common = require("../common");
var import_prompt = require("../prompt");
async function createEngineRequests(toolCalls, itemIndex, tools) {
  return toolCalls.map((toolCall) => {
    const foundTool = tools.find((tool) => tool.name === toolCall.tool);
    if (!foundTool) return;
    const nodeName = foundTool.metadata?.sourceNodeName;
    const input = foundTool.metadata?.isFromToolkit ? { ...toolCall.toolInput, tool: toolCall.tool } : toolCall.toolInput;
    return {
      nodeName,
      input,
      type: import_n8n_workflow.NodeConnectionTypes.AiTool,
      id: toolCall.toolCallId,
      metadata: {
        itemIndex
      }
    };
  });
}
function getAllTools(model, tools) {
  const modelTools = model.metadata?.tools ?? [];
  const allTools = [...tools, ...modelTools];
  return allTools;
}
function createAgentSequence(model, tools, prompt, _options, outputParser, memory, fallbackModel) {
  const agent = (0, import_agents.createToolCallingAgent)({
    llm: model,
    tools: getAllTools(model, tools),
    prompt,
    streamRunnable: false
  });
  let fallbackAgent;
  if (fallbackModel) {
    fallbackAgent = (0, import_agents.createToolCallingAgent)({
      llm: fallbackModel,
      tools: getAllTools(fallbackModel, tools),
      prompt,
      streamRunnable: false
    });
  }
  const runnableAgent = import_runnables.RunnableSequence.from([
    fallbackAgent ? agent.withFallbacks([fallbackAgent]) : agent,
    (0, import_common.getAgentStepsParser)(outputParser, memory),
    import_common.fixEmptyContentMessage
  ]);
  runnableAgent.singleAction = true;
  runnableAgent.streamRunnable = false;
  return runnableAgent;
}
async function processEventStream(ctx, eventStream, itemIndex, returnIntermediateSteps = false, memory, input) {
  const agentResult = {
    output: ""
  };
  if (returnIntermediateSteps) {
    agentResult.intermediateSteps = [];
  }
  const toolCalls = [];
  ctx.sendChunk("begin", itemIndex);
  for await (const event of eventStream) {
    switch (event.event) {
      case "on_chat_model_stream":
        const chunk = event.data?.chunk;
        if (chunk?.content) {
          const chunkContent = chunk.content;
          let chunkText = "";
          if (Array.isArray(chunkContent)) {
            for (const message of chunkContent) {
              if (message?.type === "text") {
                chunkText += message?.text;
              }
            }
          } else if (typeof chunkContent === "string") {
            chunkText = chunkContent;
          }
          ctx.sendChunk("item", itemIndex, chunkText);
          agentResult.output += chunkText;
        }
        break;
      case "on_chat_model_end":
        if (event.data) {
          const chatModelData = event.data;
          const output = chatModelData.output;
          if (output?.tool_calls && output.tool_calls.length > 0) {
            for (const toolCall of output.tool_calls) {
              toolCalls.push({
                tool: toolCall.name,
                toolInput: toolCall.args,
                toolCallId: toolCall.id || "unknown",
                type: toolCall.type || "tool_call",
                log: output.content || `Calling ${toolCall.name} with input: ${JSON.stringify(toolCall.args)}`,
                messageLog: [output]
              });
            }
            if (returnIntermediateSteps) {
              for (const toolCall of output.tool_calls) {
                agentResult.intermediateSteps.push({
                  action: {
                    tool: toolCall.name,
                    toolInput: toolCall.args,
                    log: output.content || `Calling ${toolCall.name} with input: ${JSON.stringify(toolCall.args)}`,
                    messageLog: [output],
                    // Include the full LLM response
                    toolCallId: toolCall.id || "unknown",
                    type: toolCall.type || "tool_call"
                  }
                });
              }
            }
          }
        }
        break;
      case "on_tool_end":
        if (returnIntermediateSteps && event.data && agentResult.intermediateSteps.length > 0) {
          const toolData = event.data;
          const matchingStep = agentResult.intermediateSteps.find(
            (step) => !step.observation && step.action.tool === event.name
          );
          if (matchingStep) {
            matchingStep.observation = toolData.output || "";
          }
        }
        break;
      default:
        break;
    }
  }
  ctx.sendChunk("end", itemIndex);
  if (memory && input && agentResult.output) {
    await memory.saveContext({ input }, { output: agentResult.output });
  }
  if (toolCalls.length > 0) {
    agentResult.toolCalls = toolCalls;
  }
  return agentResult;
}
function buildSteps(response, itemIndex) {
  const steps = [];
  if (response) {
    const responses = response?.actionResponses ?? [];
    if (response.metadata?.previousRequests) {
      steps.push(...response.metadata.previousRequests);
    }
    for (const tool of responses) {
      if (tool.action?.metadata?.itemIndex !== itemIndex) continue;
      const toolInput = {
        ...tool.action.input,
        id: tool.action.id
      };
      if (!toolInput || !tool.data) {
        continue;
      }
      const step = steps.find((step2) => step2.action.toolCallId === toolInput.id);
      if (step) {
        continue;
      }
      const syntheticAIMessage = new import_messages.AIMessage({
        content: `Calling ${tool.action.nodeName} with input: ${JSON.stringify(toolInput)}`,
        tool_calls: [
          {
            id: toolInput?.id ?? "reconstructed_call",
            name: (0, import_n8n_workflow.nodeNameToToolName)(tool.action.nodeName),
            args: toolInput,
            type: "tool_call"
          }
        ]
      });
      const toolResult = {
        action: {
          tool: (0, import_n8n_workflow.nodeNameToToolName)(tool.action.nodeName),
          toolInput: toolInput.input || {},
          log: toolInput.log || syntheticAIMessage.content,
          messageLog: [syntheticAIMessage],
          toolCallId: toolInput?.id,
          type: toolInput.type || "tool_call"
        },
        observation: JSON.stringify(tool.data?.data?.ai_tool?.[0]?.map((item) => item?.json) ?? "")
      };
      steps.push(toolResult);
    }
  }
  return steps;
}
async function toolsAgentExecute(response) {
  this.logger.debug("Executing Tools Agent V3");
  const returnData = [];
  let request = void 0;
  const items = this.getInputData();
  const batchSize = this.getNodeParameter("options.batching.batchSize", 0, 1);
  const delayBetweenBatches = this.getNodeParameter(
    "options.batching.delayBetweenBatches",
    0,
    0
  );
  const needsFallback = this.getNodeParameter("needsFallback", 0, false);
  const memory = await (0, import_common.getOptionalMemory)(this);
  const model = await (0, import_common.getChatModel)(this, 0);
  (0, import_node_assert.default)(model, "Please connect a model to the Chat Model input");
  const fallbackModel = needsFallback ? await (0, import_common.getChatModel)(this, 1) : null;
  if (needsFallback && !fallbackModel) {
    throw new import_n8n_workflow.NodeOperationError(
      this.getNode(),
      "Please connect a model to the Fallback Model input or disable the fallback option"
    );
  }
  for (let i = 0; i < items.length; i += batchSize) {
    const batch = items.slice(i, i + batchSize);
    const batchPromises = batch.map(async (_item, batchItemIndex) => {
      const itemIndex = i + batchItemIndex;
      if (response && response?.metadata?.itemIndex === itemIndex) {
        return null;
      }
      const steps = buildSteps(response, itemIndex);
      const input = (0, import_helpers.getPromptInputByType)({
        ctx: this,
        i: itemIndex,
        inputKey: "text",
        promptTypeKey: "promptType"
      });
      if (input === void 0) {
        throw new import_n8n_workflow.NodeOperationError(this.getNode(), 'The "text" parameter is empty.');
      }
      const outputParser2 = await (0, import_N8nOutputParser.getOptionalOutputParser)(this, itemIndex);
      const tools = await (0, import_common.getTools)(this, outputParser2);
      const options = this.getNodeParameter("options", itemIndex);
      if (options.enableStreaming === void 0) {
        options.enableStreaming = true;
      }
      const messages = await (0, import_common.prepareMessages)(this, itemIndex, {
        systemMessage: options.systemMessage,
        passthroughBinaryImages: options.passthroughBinaryImages ?? true,
        outputParser: outputParser2
      });
      const prompt = (0, import_common.preparePrompt)(messages);
      const executor = createAgentSequence(
        model,
        tools,
        prompt,
        options,
        outputParser2,
        memory,
        fallbackModel
      );
      const invokeParams = {
        steps,
        input,
        system_message: options.systemMessage ?? import_prompt.SYSTEM_MESSAGE,
        formatting_instructions: "IMPORTANT: For your response to user, you MUST use the `format_final_json_response` tool with your complete answer formatted according to the required schema. Do not attempt to format the JSON manually - always use this tool. Your response will be rejected if it is not properly formatted through this tool. Only use this tool once you are ready to provide your final answer."
      };
      const executeOptions = { signal: this.getExecutionCancelSignal() };
      const isStreamingAvailable = "isStreaming" in this ? this.isStreaming?.() : void 0;
      if ("isStreaming" in this && options.enableStreaming && isStreamingAvailable && this.getNode().typeVersion >= 2.1) {
        let chatHistory = void 0;
        if (memory) {
          chatHistory = await loadChatHistory(memory, model, options.maxTokensFromMemory);
        }
        const eventStream = executor.streamEvents(
          {
            ...invokeParams,
            chat_history: chatHistory
          },
          {
            version: "v2",
            ...executeOptions
          }
        );
        const result = await processEventStream(
          this,
          eventStream,
          itemIndex,
          options.returnIntermediateSteps,
          memory,
          input
        );
        if (result.toolCalls && result.toolCalls.length > 0) {
          const currentIteration = (response?.metadata?.iterationCount ?? 0) + 1;
          if (options.maxIterations && currentIteration > options.maxIterations) {
            throw new import_n8n_workflow.NodeOperationError(this.getNode(), "Maximum iterations reached");
          }
          const actions = await createEngineRequests(result.toolCalls, itemIndex, tools);
          return {
            actions,
            metadata: {
              previousRequests: buildSteps(response, itemIndex),
              iterationCount: currentIteration
            }
          };
        }
        return result;
      } else {
        let chatHistory = void 0;
        if (memory) {
          chatHistory = await loadChatHistory(memory, model, options.maxTokensFromMemory);
        }
        const modelResponse = await executor.invoke({
          ...invokeParams,
          chat_history: chatHistory
        });
        if ("returnValues" in modelResponse) {
          if (memory && input && modelResponse.returnValues.output) {
            let fullOutput = modelResponse.returnValues.output;
            if (steps.length > 0) {
              const toolContext = steps.map(
                (step) => `Tool: ${step.action.tool}, Input: ${JSON.stringify(step.action.toolInput)}, Result: ${step.observation}`
              ).join("; ");
              fullOutput = `[Used tools: ${toolContext}] ${fullOutput}`;
            }
            await memory.saveContext({ input }, { output: fullOutput });
          }
          const result = { ...modelResponse.returnValues };
          if (options.returnIntermediateSteps && steps.length > 0) {
            result.intermediateSteps = steps;
          }
          return result;
        }
        const currentIteration = (response?.metadata?.iterationCount ?? 0) + 1;
        if (options.maxIterations && currentIteration > options.maxIterations) {
          throw new import_n8n_workflow.NodeOperationError(this.getNode(), "Maximum iterations reached");
        }
        const actions = await createEngineRequests(modelResponse, itemIndex, tools);
        return {
          actions,
          metadata: {
            previousRequests: buildSteps(response, itemIndex),
            iterationCount: currentIteration
          }
        };
      }
    });
    const batchResults = await Promise.allSettled(batchPromises);
    const outputParser = await (0, import_N8nOutputParser.getOptionalOutputParser)(this, 0);
    batchResults.forEach((result, index) => {
      const itemIndex = i + index;
      if (result.status === "rejected") {
        const error = result.reason;
        if (this.continueOnFail()) {
          returnData.push({
            json: { error: error.message },
            pairedItem: { item: itemIndex }
          });
          return;
        } else {
          throw new import_n8n_workflow.NodeOperationError(this.getNode(), error);
        }
      }
      const response2 = result.value;
      if ("actions" in response2) {
        if (!request) {
          request = {
            actions: response2.actions,
            metadata: response2.metadata
          };
        } else {
          request.actions.push(...response2.actions);
        }
        return;
      }
      if (memory && outputParser) {
        const parsedOutput = (0, import_n8n_workflow.jsonParse)(
          response2.output
        );
        response2.output = parsedOutput?.output ?? parsedOutput;
      }
      const itemResult = {
        json: (0, import_omit.default)(
          response2,
          "system_message",
          "formatting_instructions",
          "input",
          "chat_history",
          "agent_scratchpad"
        ),
        pairedItem: { item: itemIndex }
      };
      returnData.push(itemResult);
    });
    if (i + batchSize < items.length && delayBetweenBatches > 0) {
      await (0, import_n8n_workflow.sleep)(delayBetweenBatches);
    }
  }
  if (request) {
    return request;
  }
  return [returnData];
}
async function loadChatHistory(memory, model, maxTokensFromMemory) {
  const memoryVariables = await memory.loadMemoryVariables({});
  let chatHistory = memoryVariables["chat_history"];
  if (maxTokensFromMemory) {
    chatHistory = await (0, import_messages.trimMessages)(chatHistory, {
      strategy: "last",
      maxTokens: maxTokensFromMemory,
      tokenCounter: model,
      includeSystem: true,
      startOn: "human",
      allowPartial: true
    });
  }
  return chatHistory;
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  toolsAgentExecute
});
//# sourceMappingURL=execute.js.map